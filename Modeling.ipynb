{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import config as user_config\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertForSequenceClassification(BertPreTrainedModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.num_labels = config.num_labels\n",
    "\n",
    "#         self.bert = BertModel(config)\n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "#         self.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classfiy(nn.Module):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
    "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "\n",
    "    num_labels = 2\n",
    "\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *arg):\n",
    "        super(classfiy, self).__init__()\n",
    "        self.result = {}\n",
    "        num_labels, total_steps, _ = arg\n",
    "        self.bert = BertModel.from_pretrained(user_config.pretrained)\n",
    "        config = BertConfig.from_pretrained(user_config.pretrained)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.labels_data = None\n",
    "        \n",
    "#         #self.cls = BertOnlyMLMHead(config)\n",
    "#         def init_weights(module):\n",
    "#             if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "#                 # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "#                 # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "#                 module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "#             elif isinstance(module, BERTLayerNorm):\n",
    "#                 module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "#                 module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "#             if isinstance(module, nn.Linear):\n",
    "#                 module.bias.data.zero_()\n",
    "                \n",
    "        def _init_weights(self, module):\n",
    "            \"\"\" Initialize the weights \"\"\"\n",
    "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            elif isinstance(module, BertLayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, data, *arg):\n",
    "        labels = None\n",
    "        # print(len(data))\n",
    "        if len(data) == 4:\n",
    "            input_ids, attention_mask, token_type_ids, labels = data\n",
    "        else:\n",
    "            input_ids, attention_mask, token_type_ids = data\n",
    "        if arg is not ():\n",
    "            self.global_step, _ = arg\n",
    "        self.labels_data = labels\n",
    "        pooled_output = self.encoder(input_ids, attention_mask, token_type_ids)\n",
    "        output = self.decoder(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss = self.calc_loss(output, labels)\n",
    "            return loss, torch.nn.functional.softmax(output, dim=-1)\n",
    "        return torch.nn.functional.softmax(output, dim=-1)\n",
    "\n",
    "    def encoder(self, *x):\n",
    "        input_ids, attention_mask, token_type_ids = x\n",
    "        all_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        return pooled_output\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def calc_loss(self, inputs, targets):\n",
    "        loss_cross_entropy = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        loss_cross_entropy = loss_cross_entropy(inputs, targets)\n",
    "        self.result['loss'] = loss_cross_entropy.detach().item()\n",
    "        self.result['accuracy'] = self.calc_accuracy(inputs.detach().cpu().numpy(), targets.detach().cpu().numpy())\n",
    "        self.result['f1'] = self.calc_f1(inputs.detach().cpu().numpy(), targets.detach().cpu().numpy())\n",
    "        return loss_cross_entropy\n",
    "\n",
    "    def calc_accuracy(self, inputs, targets):\n",
    "        outputs = np.argmax(inputs, axis=1)\n",
    "        return np.mean(outputs == targets)\n",
    "\n",
    "    def calc_f1(self, inputs, targets):\n",
    "        outputs = np.argmax(inputs, axis=1)\n",
    "        return f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    def get_result(self):\n",
    "        return self.result\n",
    "\n",
    "    def get_labels_data(self):\n",
    "        return self.labels_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
