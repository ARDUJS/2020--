{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 预训练模型数据读取模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 处理本地数据集合 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.1 切分数据集\n",
    "- 以9:1的比例切分数据为 训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 27768\n",
      "labels: ['angry', 'fear', 'happy', 'neural', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "### 切分数据集为训练集和验证集\n",
    "import random\n",
    "random.seed(1)\n",
    "labels = set()\n",
    "fp_dev = open(\"data/train/train/usual_dev.tsv\", \"w+\", encoding='utf-8')\n",
    "fp = open(\"data/train/train/usual_train.tsv\", \"w+\", encoding='utf-8')\n",
    "\n",
    "file_in = open(\"data/train/train/usual_train.txt\", \"rb\")\n",
    "lines = file_in.read().decode(\"utf-8\") \n",
    "lines = eval(lines)\n",
    "\n",
    "print(\"total:\", len(lines))\n",
    "for line in lines:\n",
    "    labels.add(line['label'])\n",
    "    if random.random() > 0.1:\n",
    "        fp.write(str(line['id']) + '\\t' + line['content'] + '\\t' + line['label'] + '\\n')\n",
    "    else:\n",
    "        fp_dev.write(str(line['id']) + '\\t' + line['content'] + '\\t' + line['label'] + '\\n')\n",
    "fp.close()\n",
    "labels = list(labels)\n",
    "labels.sort()\n",
    "print(\"labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Test\n",
    "- 将原test数据集处理为tsv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 修改test集\n",
    "fp_test_1 = open(\"data/train/train/usual_test.tsv\", \"w+\", encoding='utf-8')\n",
    "fp_in = open(\"data/eval/eval/usual_eval.txt\", \"rb\")\n",
    "lines = fp_in.read().decode(\"utf-8\") \n",
    "lines = eval(lines)\n",
    "for line in lines:\n",
    "    fp_test_1.write(str(line['id']) + '\\t' + line['content'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 读取本地数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import config\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 定义存储数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 定义数据处理类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        file_in = open(input_file, \"rb\")\n",
    "        lines = []\n",
    "        for line in file_in:\n",
    "            lines.append(line.decode(\"utf-8\").split(\"\\t\"))\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the Discourage data set .\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.labels = set()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"usual_train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"usual_train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"usual_dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"usual_test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        tmp = list(self.labels)\n",
    "        tmp.sort()\n",
    "        return tmp\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[1].strip()\n",
    "            if len(line) < 3:\n",
    "                label = None\n",
    "            else:\n",
    "                label = line[2].strip()\n",
    "            text_b = None\n",
    "            self.labels.add(label)\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 定义数据转换为特征函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        if example.label is not None:\n",
    "            label_id = label_map[example.label]\n",
    "        else:\n",
    "            label_id = None\n",
    "        if ex_index < 2:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [x for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            if example.label is not None:\n",
    "                logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(\n",
    "                        input_ids=input_ids,\n",
    "                        input_mask=input_mask,\n",
    "                        segment_ids=segment_ids,\n",
    "                        label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 定义截断函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 定义生成数据类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# 按这个顺序 all_input_ids, all_input_mask, all_segment_ids, all_label_ids 生成数据\n",
    "####\n",
    "class DataGenerate():\n",
    "    def __init__(self):\n",
    "        self.processor = ClassifyProcessor()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "        self.train_examples = None\n",
    "        self.num_train_steps = None\n",
    "        \n",
    "        self.train_data_loader = None\n",
    "        self.test_data_loader = None\n",
    "        self.dev_data_loader = None\n",
    "        \n",
    "#         self.get_train_loader()\n",
    "        pass\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        if self.train_data_loader is None:\n",
    "            if self.train_examples is None:\n",
    "                self.train_examples = self.processor.get_train_examples(config.data_dir)\n",
    "            label_list = self.processor.get_labels()\n",
    "            self.labels_list = label_list\n",
    "            train_features = convert_examples_to_features(\n",
    "                self.train_examples, self.get_labels(), config.max_seq_length, self.tokenizer)\n",
    "            self.num_train_steps = int(\n",
    "                len(self.train_examples) / config.train_batch_size\n",
    "                / config.gradient_accumulation_steps * config.num_train_epochs)\n",
    "            logger.info(\"***** Running training *****\")\n",
    "            logger.info(\"  Num examples = %d\", len(self.train_examples))\n",
    "            logger.info(\"  Batch size = %d\", config.train_batch_size)\n",
    "            logger.info(\"  Num steps = %d\", self.num_train_steps)\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "            all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "            train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "                \n",
    "            train_sampler = RandomSampler(train_data)\n",
    "            self.train_data_loader = DataLoader(train_data, sampler=train_sampler, batch_size=config.train_batch_size)\n",
    "        return self.train_data_loader\n",
    "\n",
    "    def get_dev_loader(self):\n",
    "        if self.dev_data_loader is None:\n",
    "            dev_examples = self.processor.get_dev_examples(config.data_dir)\n",
    "            dev_features = convert_examples_to_features(\n",
    "                dev_examples, self.get_labels(), config.max_seq_length, self.tokenizer)\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in dev_features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in dev_features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in dev_features], dtype=torch.long)\n",
    "            all_label_ids = torch.tensor([f.label_id for f in dev_features], dtype=torch.long)\n",
    "            dev_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "            self.dev_data_loader = DataLoader(dev_data, batch_size=config.dev_batch_size)\n",
    "            \n",
    "        return self.dev_data_loader\n",
    "\n",
    "    def get_test_loader(self):\n",
    "        if self.test_data_loader is None:\n",
    "            test_examples = self.processor.get_test_examples(config.data_dir)\n",
    "            test_features = convert_examples_to_features(\n",
    "                test_examples, self.get_labels(), config.max_seq_length, self.tokenizer)\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "            # all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "            test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "            self.test_data_loader = DataLoader(test_data, batch_size=config.test_batch_size)\n",
    "            \n",
    "        return self.test_data_loader\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels_list\n",
    "\n",
    "    def get_num_train_steps(self):\n",
    "        return self.num_train_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 实例化数据类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/18/2020 13:56:37 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at C:\\Users\\Administrator\\.cache\\torch\\transformers\\8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
      "07/18/2020 13:56:37 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "07/18/2020 13:56:38 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at C:\\Users\\Administrator\\.cache\\torch\\transformers\\8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "data_generator = DataGenerate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# 按这个顺序 all_input_ids, all_input_mask, all_segment_ids, all_label_ids 生成数据\n",
    "####\n",
    "train_loader = data_generator.get_train_loader()\n",
    "dev_loader = data_generator.get_dev_loader()\n",
    "test_loader = data_generator.get_test_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 展示训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当 你 变 优 秀 时 ， 你 想 要 的 都 会 来 找 你\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "it = next(iter(dev_loader))\n",
    "all_input_ids, all_input_mask, all_segment_ids, all_label_ids = it\n",
    "print(data_generator.tokenizer.decode(all_input_ids[0], skip_special_tokens=True))\n",
    "print(data_generator.get_labels()[all_label_ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method decode in module transformers.tokenization_utils:\n",
      "\n",
      "decode(token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True) method of transformers.tokenization_bert.BertTokenizer instance\n",
      "    Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n",
      "    with options to remove special tokens and clean up tokenization spaces.\n",
      "    Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      "    \n",
      "    Args:\n",
      "        token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n",
      "        skip_special_tokens: if set to True, will replace special tokens.\n",
      "        clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data_generator.tokenizer.decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 展示预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所 以 注 定 我 这 辈 子 是 做 不 了 商 人 妈 蛋\n"
     ]
    }
   ],
   "source": [
    "it = next(iter(test_loader))\n",
    "all_input_ids, all_input_mask, all_segment_ids = it\n",
    "print(data_generator.tokenizer.decode(all_input_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
